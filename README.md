# LLM Text Summarization Comparative Analysis

## Сравнительный анализ моделей суммаризации текста

Комплексное исследование качества суммаризации текста с использованием больших языковых моделей (LLM) на примере анализа статьи "2025 год: LLM покоряют мир".

## Цель проекта

Провести объективное сравнение современных LLM моделей для задач суммаризации текста на русском языке с использованием структурированных метрик качества.

## Основные результаты

Детальный анализ представлен в финальном отчете: [`reports/FINAL_STRUCTURED_REPORT.md`](reports/FINAL_STRUCTURED_REPORT.md)

### Протестированные модели:
- **Claude 3.5 Sonnet** (Anthropic) - 86.38 баллов (лучший результат)
- **Groq Llama3-8B** (реальный API) - 68.49 баллов

### Ключевые метрики оценки:
- **Faithfulness** (30%) - верность исходному тексту
- **Coverage** (25%) - полнота охвата ключевых тем  
- **Prompt Adherence** (20%) - следование инструкциям
- **Coherence** (15%) - связность текста
- **Compression Ratio** (10%) - эффективность сжатия

## Быстрый старт

### Предварительные требования

```bash
# Python 3.8+
pip install -r requirements.txt
```

### Настройка API ключей

1. Скопируйте `env.example` в `.env`
2. Добавьте ваш Groq API ключ:
```bash
export GROQ_API_KEY='your_groq_api_key_here'
```

### Запуск анализа

```bash
# 1. Генерация суммаризации с использованием Groq API
python groq_new_prompt.py

# 2. Анализ качества суммаризаций
python src/llm_analyzer.py

# 3. Генерация полного структурированного отчета
python generate_report.py

# 4. Просмотр финального отчета
python view_report.py
```

## Структура проекта

```
├── reports/
│   └── FINAL_STRUCTURED_REPORT.md  # Финальный отчет исследования
├── results/                        # Результаты суммаризаций
│   ├── new_claude_summary.md       # Суммаризация Claude 3.5
│   ├── groq_new_result.json       # Результаты Groq API
│   ├── groq_new_summary.txt       # Суммаризация Groq
│   └── ...
├── data/
│   └── source_article.txt         # Исходная статья для анализа
├── src/
│   ├── __init__.py               # Инициализация модуля
│   └── llm_analyzer.py           # Модуль анализа качества
├── metrics/                      # Метрики анализа
├── groq_new_prompt.py           # Скрипт для тестирования Groq
├── generate_report.py           # Генератор финального отчета
├── view_report.py              # Просмотр статистики финального отчета в консоли
├── requirements.txt            # Зависимости проекта
├── env.example                 # Пример настроек окружения
├── .gitignore                  # Игнорируемые файлы
├── LICENSE                     # MIT лицензия
├── CONTRIBUTING.md             # Руководство для контрибьюторов
└── README.md                   # Этот файл
```

## Основные компоненты

### `groq_new_prompt.py`
Скрипт для получения суммаризации от Groq API с использованием структурированного промта.

### `src/llm_analyzer.py`
Модуль для анализа качества суммаризаций по 5 ключевым метрикам.

### `generate_report.py`
Генератор финального структурированного отчета в формате a) b) c) d) согласно заданию.

### `view_report.py`
Утилита для просмотра статистики финального отчета в консоли.

## Методология исследования

1. **Единый структурированный промт** для всех моделей
2. **Объективные метрики** оценки качества
3. **Реальное тестирование** на API (не симуляция)
4. **Двойная проверка** всех результатов

## Основные выводы

**Claude 3.5 Sonnet** продемонстрировал наилучшие результаты:
- [OK] Отсутствие галлюцинаций (100%)
- [OK] Идеальное следование промту
- [OK] Высокое покрытие ключевых тем (80%)
- [OK] Эффективное сжатие информации

Рекомендуется как основной инструмент для профессиональных задач суммаризации в 2025 году.

## Технологии

- Python 3.8+
- Groq API
- Структурированный анализ метрик
- Markdown для документации

## Лицензия

MIT License - см. [LICENSE](LICENSE) для деталей.

## Вклад в проект

Приветствуются улучшения! См. [CONTRIBUTING.md](CONTRIBUTING.md) для руководства.

## Контакты

Вопросы и предложения направляйте через [Issues](https://github.com/yourusername/llm-summarization-analysis/issues).

---

**Дата исследования:** 07.01.2025  
**Статус:** [OK] Завершено 