#!/usr/bin/env python3
"""
Скрипт для генерации суммаризации с использованием Groq API.
Использует структурированный промт для получения качественной суммаризации.
"""

import json
import os
from datetime import datetime
from groq import Groq


def main():
    """Основная функция для генерации суммаризации."""
    # Загрузка API ключа из переменной окружения
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        print("[ERROR] GROQ_API_KEY environment variable not set")
        print("Please set it using: export GROQ_API_KEY='your_key_here'")
        return 1
    
    # Структурированный промт
    prompt = """Ты — профессиональный аналитик, специализирующийся на создании 
точных и лаконичных текстовых сводок.

Твоя задача: создать **структурированную** суммаризацию предоставленной статьи 
на **русском языке**, строго соблюдая требования ниже.

**Исходный материал:**
https://www.itweek.ru/ai/article/detail.php?ID=231899

**Строгие Требования к Суммаризации:**
1.  **Объем:** Ровно 100-150 слов. (*Точное соответствие критично*)
2.  **Содержание:**
    *   **Ключевая суть:** Четко выдели основную мысль/цель/вывод статьи.
    *   **Факты и цифры:** Включи **все** значимые факты, статистику, даты, 
        результаты исследований. Сохрани точные числовые значения.
    *   **Имена и названия:** Сохрани **точные** названия компаний, 
        организаций, продуктов, моделей устройств, имена ключевых персон.
    *   **Термины:** Сохрани важные технические или специфические термины. 
        При необходимости кратко поясни их в контексте.
3.  **Структура (Обязательно):** Используй пронумерованный список для ясности:
    *   **1. Основная тема и цель:** (1-2 предложения)
    *   **2. Ключевые факты и данные:** (Перечисли важнейшие цифры, события, 
        названия)
    *   **3. Важные выводы или последствия:** (Основной результат, прогноз, 
        значение)
4.  **Стиль и Качество:**
    *   **Нейтрально-аналитический тон:** Без эмоций, оценок, домыслов.
    *   **Связность:** Текст должен быть гладким, логичным, а не просто набором 
        пунктов.
    *   **Ясность и точность:** Максимально понятный язык без потери точности 
        деталей.
    *   **Источник:** Не добавляй информацию, отсутствующую в исходной статье. 
        **Запрещены домыслы.**
    *   **Грамматика и орфография:** Безупречный русский язык.

**Цель:** Создать самодостаточную выжимку, позволяющую быстро понять основное 
содержание и ключевые детали исходной статьи, соответствующую профессиональным 
стандартам аналитики 2025 года.

**Текст статьи:**
2025 год: LLM покоряют мир

Большие языковые модели (LLM, Large Language Models) трансформируют 
бизнес-процессы, автоматизируя работу с данными и взаимодействие с клиентами. 
В этом материале мы представим собственную оценку, какие LLM подходят для 
бизнеса и чем они отличаются.

В 2020-е разработка больших языковых моделей превратилась в глобальную 
технологическую гонку, в которой участвуют крупнейшие компании и научные 
центры мира. Ключевые игроки стремятся создать более точные, производительные 
и доступные модели, способные решать сложные задачи в разных сферах. 
По оценкам Gartner и McKinsey, рынок ИИ будет расти со среднегодовым темпом 
около 36% и к 2030 году может достичь 1,75 трлн. долл. При этом темп роста 
сегмента LLM и генеративного ИИ ожидается свыше 40%.

Тренды развития современных LLM

В последние месяцы рынок генеративного ИИ пережил значительный скачок в 
развитии: OpenAI выпустила новую модель ChatGPT o3-mini, а китайская компания 
DeepSeek представила DeepSeek R1, которая показала сопоставимые результаты при 
более низкой стоимости разработки. Alibaba выпустила модель Qwen 2.5 Max, 
заявив, что она превосходит по производительности модели от DeepSeek и OpenAI, 
а Илон Маск, стремясь конкурировать с OpenAI, основал компанию xAI, которая 
представила чат-бота Grok. Эти события вновь разогрели дискуссию о будущем 
LLM и их применении в бизнесе, а также привели к новой волне тестирований.

Рассмотрим основные тренды развития LLM:

1. Оптимизация вычислений для лучшего результата при меньших затратах

Ранее рост мощности LLM определялся увеличением количества параметров модели, 
что требовало значительных вычислительных ресурсов. Однако последние релизы 
показали, что эффективность можно повышать и без экстремального роста объема 
модели. Так, DeepSeek R1 продемонстрировала сопоставимую с GPT-4 
производительность при значительно меньших затратах на обучение. В ответ 
OpenAI ускорила выпуск o3, а Google представила обновленную Gemini 2.0.

2. Расширение контекстного окна и глубины размышлений

Количество токенов, которые модель может учитывать в одном запросе, стало 
критически важным показателем. Если раньше стандарт составлял 128K, то теперь 
компании стремятся увеличить этот параметр для более сложных задач. Сегодня 
Google Gemini 2.0 Flash предлагает уже 1 млн. токенов, что позволяет моделям 
работать с большими объемами информации, включая анализ длинных документов, 
сложных кодов и диалогов.

Современные модели переходят от статичной генерации к гибкой системе 
размышлений. В OpenAI o3-mini, DeepSeek R1 и других передовых LLM появились 
уровни анализа запроса: быстрый режим (мгновенный ответ при минимальных 
вычислительных затратах), стандартный режим (баланс между скоростью и 
точностью) и глубокий режим (модель анализирует запрос пошагово, используя 
метод Chain-of-Thought), который позволяет находить более точные решения.

3. Фокус на безопасность и сложные технические задачи

Новейшая модель OpenAI o3-mini была протестирована на самых сложных бенчмарках, 
включая Humanities Last Exam, где она получила наибольший на сегодняшний день 
результат в 13% среди самых последних коммерческих моделей. Этот тест 
проверяет способность модели решать задачи, требующие глубокого понимания 
логики, аргументации и знаний в разных областях.

Кроме того, OpenAI значительно улучшила защитные механизмы модели: перед 
генерацией ответа она анализирует запрос и пытается выявить попытки 
манипуляций и обмана. Модель способна отказать в ответе или предложить 
альтернативное объяснение, если пользователь задает вопросы на запрещенные 
темы. Еще одно улучшение — интеграция с функциями поиска информации в 
Интернете, которая помогает повысить точность фактов.

4. Развитие open-source LLM как альтернативы коммерческим моделям

Llama 3 от Meta и DeepSeek R1 уже доказывают, что open-source решения могут 
конкурировать с коммерческими LLM по качеству, при этом их можно адаптировать 
для корпоративных нужд.

5. Развитие мультиязычных возможностей

Хотя OpenAI и другие западные компании продолжают доминировать в сфере 
мультиязычных LLM, локальные модели набирают силу. Особенно это актуально для 
стран с ограниченным доступом к зарубежным решениям. Российские LLM, такие как 
YandexGPT, GigaChat и Cotype, активно дообучаются, хотя пока и уступают 
западным аналогам в ряде областей.

Как выбрать LLM для бизнеса

Компании, выбирающие LLM для внедрения, должны учитывать не только 
маркетинговые обещания, но и реальные показатели тестирования. 
Производительность на бенчмарках, глубина анализа, возможность контроля за 
выдаваемыми ответами, кастомизации и интеграции, стоимость использования и 
юридические ограничения — ключевые параметры, которые определяют, насколько 
эффективной будет интеграция ИИ в бизнес-процессы.

С развитием языковых моделей бизнес сталкивается с новым вызовом: 
использование мощных LLM в их общем виде не всегда эффективно. Создание 
собственных LLM дает бизнесу ряд преимуществ, связанных с безопасностью, 
точностью и экономической эффективностью. Компании получают полный контроль 
над данными, обучая модели на внутренних корпоративных источниках, что 
гарантирует соответствие отраслевым стандартам и требованиям безопасности.

Кастомизированные модели значительно снижают вероятность генеративных ошибок 
и «галлюцинаций», так как адаптируются под узкоспециализированные задачи, а 
также легко интегрируются с корпоративными системами, включая CRM, ERP и базы 
знаний. В России такой подход также становится необходимостью из-за требований 
законодательства, накладывающего определенные ограничения на использование 
западных моделей.

Гибридный подход для бизнеса

Несмотря на растущий интерес к собственным моделям, гибридный подход к 
использованию LLM в бизнесе может быстро и существенно повысить эффективность 
работы при адекватных финансовых вложениях. Гибридный подход, реализованный, 
например, в диалоговой платформе для разработки коммуникативного ИИ, сочетает 
в себе преимущества двух архитектур: в платформе создается четкая 
информационная база, которая служит основным источником фактов для модели. 
Генеративная модель берет на себя задачу ведения беседы, формулирования 
ответов и адаптации к стилю общения пользователя. При этом платформа задает 
четкие рамки, регулируя, на какие темы и в каком формате может отвечать LLM.

Очевидно, что рынок LLM вступает в фазу высокой технологической 
турбулентности, где ключевым фактором успеха станет не просто доступ к 
передовым моделям, а способность адаптировать их под конкретные бизнес- и 
социальные задачи, обеспечивая при этом надежность, управляемость и 
соответствие регуляторным требованиям.

Сравнение ключевых LLM

Ниже представлены ключевые особенности ведущих LLM и их отличия.

Claude (Anthropic) - контекстное окно 200K токенов, фокус на безопасность и 
интерпретируемость ответов, улучшенная работа с визуальными задачами, высокая 
персонализация взаимодействия с пользователем. Доступность в России 
ограниченная.

GPT-4/ChatGPT (OpenAI) - контекстное окно 128K токенов, самая популярная 
коммерческая модель, интеграция с GPT Search, различные уровни «размышления» 
для более точных ответов, универсальность и высокая генеративная способность. 
Доступность в России ограниченная.

Gemini (Google) - контекстное окно 1M токенов, мультимодальная обработка 
текста, аудио, видео, высокий уровень фактчекинга. Доступность в России 
ограниченная.

YandexGPT (Yandex) - контекстное окно 100K токенов, локализация под российский 
рынок, интеграция с отечественными сервисами, соответствие российскому 
законодательству. Полная доступность в России.

GigaChat (Сбер) - контекстное окно 200K токенов, поддержка корпоративных и 
банковских задач, глубокая адаптация под локальные бизнес-процессы, 
соответствие российскому законодательству. Полная доступность в России.

Cotype (МТС) - контекстное окно 128K токенов, узкоспециализированные решения 
для бизнеса, высокая скорость обработки запросов, соответствие российскому 
законодательству. Полная доступность в России.

Llama (Meta) - контекстное окно 128K токенов, открытый исходный код, гибкость 
кастомизации, развитая open-source экосистема. Открытая доступность.

DeepSeek (Китай) - контекстное окно 128K токенов, высокая эффективность при 
низких затратах, поддержка open-source и обучения с подкреплением, высокая 
производительность при сложных задачах. Открытая доступность.

Qwen (Alibaba) - контекстное окно 200K токенов, высокая производительность при 
конкурентной цене, превосходит GPT-4o и другие ведущие модели по ряду 
параметров, интеграция с облачной платформой Alibaba. Открытая доступность."""
    
    print("[INFO] Generating summary using Groq API...")
    
    try:
        client = Groq(api_key=api_key)
        
        # Создаем запрос
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model="llama3-8b-8192",
            max_tokens=500,
            temperature=0.1
        )
        
        summary = chat_completion.choices[0].message.content
        word_count = len(summary.split())
        
        print(f"[OK] Summary generated: {word_count} words")
        
        # Сохраняем результаты в файлы
        result_data = {
            "timestamp": datetime.now().isoformat(),
            "model": "Groq Llama3-8B",
            "prompt": prompt,
            "summary": summary,
            "word_count": word_count,
            "status": "success"
        }
        
        # Создаем директорию results если её нет
        os.makedirs('results', exist_ok=True)
        
        # Сохраняем JSON
        with open('results/groq_new_result.json', 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        # Сохраняем только суммаризацию
        with open('results/groq_new_summary.txt', 'w', encoding='utf-8') as f:
            f.write(summary)
        
        # Краткий статус
        with open('results/groq_new_status.txt', 'w', encoding='utf-8') as f:
            f.write("SUCCESS: New structured prompt completed!\n")
            f.write(f"Words: {word_count}\n")
            f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        print("[OK] Results saved to results/ directory")
        return 0
            
    except Exception as e:
        print(f"[ERROR] Failed to generate summary: {str(e)}")
        
        # Создаем директорию results если её нет
        os.makedirs('results', exist_ok=True)
        
        # Сохраняем ошибку
        with open('results/groq_new_error.txt', 'w', encoding='utf-8') as f:
            f.write(f"ERROR: {str(e)}\n")
            f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        return 1


if __name__ == "__main__":
    exit(main()) 
